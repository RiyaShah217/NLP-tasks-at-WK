{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5de6e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3e243d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91e87a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b287e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rank-bm25 in c:\\users\\riya.shah\\anaconda3\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\riya.shah\\anaconda3\\lib\\site-packages (from rank-bm25) (1.21.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rank-bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efee8df",
   "metadata": {},
   "source": [
    "### Tokenization ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ea8e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jack', 'is', 'a', 'sharp', 'minded', 'fellow']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "sentence = 'Jack is a sharp minded fellow'\n",
    "words = word_tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdf1e8d",
   "metadata": {},
   "source": [
    "### Removing special characters ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "169c7a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spl_chars_removal(lst):\n",
    "    lst1=list()\n",
    "    for element in lst:\n",
    "        str=''\n",
    "        str = re.sub('[â°-9a-zA-Z]','',element)\n",
    "        lst1.append(str)\n",
    "    return lst1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8124127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding words to stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5dd8d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding custom words to the pre-defined stop words list\n",
    "all_stopwords_gensim = STOPWORDS.union(set(['disease']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85bae22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwprds_removal_gensim_custom(lst):\n",
    "    lst1=list()\n",
    "    for str in lst:\n",
    "        text_tokens = word_tokenize(str)\n",
    "        tokens_without_sw = [word for word in text_tokens if not word in all_stopwords_gensim]\n",
    "        str_t =' '.join(tokens_without_sw)\n",
    "        lst1.append(str_t)\n",
    " \n",
    "    return lst1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144da3b4",
   "metadata": {},
   "source": [
    "### Stemming ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12997110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machin\n",
      "learn\n",
      "is\n",
      "cool\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer() \n",
    "sentence = 'Machine Learning is cool'\n",
    "for word in sentence.split():\n",
    "   print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45640ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [doc.split(' ') for doc in sentence]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "query = 'vaccine' ## Enter search query\n",
    "tokenized_query = query.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b6be9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "doc_scores = bm25.get_scores(tokenized_query)\n",
    "print(doc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d102092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\riya.shah\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\riya.shah\\anaconda3\\lib\\site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\riya.shah\\anaconda3\\lib\\site-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\riya.shah\\anaconda3\\lib\\site-packages (from gensim) (5.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "430fb247",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m docs \u001b[38;5;241m=\u001b[39m bm25\u001b[38;5;241m.\u001b[39mget_top_n(tokenized_query,sentence, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m df_search \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(docs)]\n\u001b[0;32m      3\u001b[0m df_search\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "docs = bm25.get_top_n(tokenized_query,sentence, n=5)\n",
    "df_search = df[df['Text'].isin(docs)]\n",
    "df_search.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a32fea53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    'Human machine interface for lab abc computer applications',\n",
    "    'A survey of user opinion of computer system response time',\n",
    "    'The EPS user interface management system',\n",
    "    'System and human system engineering testing of EPS',\n",
    "    'Relation of user perceived response time to error measurement',\n",
    "    'The generation of random binary unordered trees',\n",
    "    'The intersection graph of paths in trees',\n",
    "    'Graph minors IV Widths of trees and well quasi ordering',\n",
    "    'Graph minors A survey'\n",
    "]\n",
    "\n",
    "# remove stop words and tokenize them (we probably want to do some more\n",
    "# preprocessing with our text in a real world setting, but we'll keep\n",
    "# it simple here)\n",
    "stopwords = set(['for', 'a', 'of', 'the', 'and', 'to', 'in'])\n",
    "texts = [\n",
    "    [word for word in document.lower().split() if word not in stopwords]\n",
    "    for document in corpus\n",
    "]\n",
    "\n",
    "# build a word count dictionary so we can remove words that appear only once\n",
    "word_count_dict = {}\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        word_count = word_count_dict.get(token, 0) + 1\n",
    "        word_count_dict[token] = word_count\n",
    "\n",
    "texts = [[token for token in text if word_count_dict[token] > 1] for text in texts]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e832b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25:\n",
    "     def __init__(self, k1=1.5, b=0.75):\n",
    "        self.b = b\n",
    "        self.k1 = k1\n",
    "\n",
    "     def fit(self, corpus):\n",
    "        tf = []\n",
    "        df = {}\n",
    "        idf = {}\n",
    "        doc_len = []\n",
    "        corpus_size = 0\n",
    "        for document in corpus:\n",
    "            corpus_size += 1\n",
    "            doc_len.append(len(document))\n",
    "\n",
    "            # compute tf (term frequency) per document\n",
    "            frequencies = {}\n",
    "            for term in document:\n",
    "                term_count = frequencies.get(term, 0) + 1\n",
    "                frequencies[term] = term_count\n",
    "\n",
    "            tf.append(frequencies)\n",
    "\n",
    "            # compute df (document frequency) per term\n",
    "            for term, _ in frequencies.items():\n",
    "                df_count = df.get(term, 0) + 1\n",
    "                df[term] = df_count\n",
    "\n",
    "        for term, freq in df.items():\n",
    "            idf[term] = math.log(1 + (corpus_size - freq + 0.5) / (freq + 0.5))\n",
    "\n",
    "        self.tf_ = tf\n",
    "        self.df_ = df\n",
    "        self.idf_ = idf\n",
    "        self.doc_len_ = doc_len\n",
    "        self.corpus_ = corpus\n",
    "        self.corpus_size_ = corpus_size\n",
    "        self.avg_doc_len_ = sum(doc_len) / corpus_size\n",
    "        return self\n",
    "\n",
    "     def search(self, query):\n",
    "        scores = [self._score(query, index) for index in range(self.corpus_size_)]\n",
    "        return scores\n",
    "\n",
    "     def _score(self, query, index):\n",
    "        score = 0.0\n",
    "\n",
    "        doc_len = self.doc_len_[index]\n",
    "        frequencies = self.tf_[index]\n",
    "        for term in query:\n",
    "            if term not in frequencies:\n",
    "                continue\n",
    "\n",
    "            freq = frequencies[term]\n",
    "            numerator = self.idf_[term] * freq * (self.k1 + 1)\n",
    "            denominator = freq + self.k1 * (1 - self.b + self.b * doc_len / self.avg_doc_len_)\n",
    "            score += (numerator / denominator)\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16fecd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\tHuman machine interface for lab abc computer applications\n",
      "0.999\tA survey of user opinion of computer system response time\n",
      "0.0\tThe EPS user interface management system\n",
      "0.0\tSystem and human system engineering testing of EPS\n",
      "0.0\tRelation of user perceived response time to error measurement\n",
      "1.522\tThe generation of random binary unordered trees\n",
      "2.532\tThe intersection graph of paths in trees\n",
      "2.167\tGraph minors IV Widths of trees and well quasi ordering\n",
      "2.514\tGraph minors A survey\n"
     ]
    }
   ],
   "source": [
    "query = 'The intersection of graph survey and trees'\n",
    "query = [word for word in query.lower().split() if word not in stopwords]\n",
    "\n",
    "bm25 = BM25()\n",
    "bm25.fit(texts)\n",
    "scores = bm25.search(query)\n",
    "\n",
    "for score, doc in zip(scores, corpus):\n",
    "    score = round(score, 3)\n",
    "    print(str(score) + '\\t' + doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d24344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
